{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from my_transformers import ClassificationTransformer\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchtext import datasets, vocab\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import random, tqdm, sys, math, gzip\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset imdb (/root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0)\n",
      "100%|███████████████████████████████████████████| 3/3 [00:00<00:00, 289.80it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "tbw = SummaryWriter()\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "\n",
    "# train, test = datasets.IMDB()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "dataset = load_dataset(\"imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "    unsupervised: Dataset({\n",
      "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 50000\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    }
   ],
   "source": [
    "\n",
    "print(dataset)\n",
    "dataset = dataset.map(lambda x: tokenizer(x['text'], padding='max_length', truncation=True, max_length=100, return_tensors='pt'), batched=True)\n",
    "dataset = dataset.map(lambda x: {\"tokens\": x['input_ids']},batched=True)\n",
    "dataset = dataset.map(lambda x: {\"labels\": x['label']}, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': tensor([[  101,  1045,  2293,  ...,  2019,  1055,   102],\n",
      "        [  101,  2023,  2524,  ...,  2095,  1010,   102],\n",
      "        [  101,  2007,  1005,  ...,  2040,  2763,   102],\n",
      "        ...,\n",
      "        [  101,  4212,  2046,  ...,  2004,  2045,   102],\n",
      "        [  101,  2043,  1045,  ...,  2191,  7078,   102],\n",
      "        [  101,  1045,  1005,  ...,  4626, 21862,   102]]), 'labels': tensor([0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,\n",
      "        1, 0, 1, 0, 1, 0, 0, 1])}\n"
     ]
    }
   ],
   "source": [
    "# dataset.train_test_split(test_size=0.1)\n",
    "dataset.set_format(type=\"torch\", columns=[\"tokens\", \"labels\"])\n",
    "# dataset = load_dataset(\"imdb\")\n",
    "# train = list(train)\n",
    "# test = list(test)\n",
    "# train = torch.utils.data.DataLoader(train, batch_size=batch_size, drop_last=True)\n",
    "# test = torch.utils.data.DataLoader(test, batch_size=batch_size, drop_last=True)\n",
    "mx = 1000\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(dataset['train'], batch_size=batch_size, shuffle=True)\n",
    "test_laoder = DataLoader(dataset['test'], batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print(next(iter(train_loader)))\n",
    "\n",
    "# def tokenize_data(data, tokenizer, max_length=30):\n",
    "    \n",
    "#     tokenized_data = []\n",
    "#     for label, text in tqdm.tqdm(data):\n",
    "#         tokens = tokenizer(text, padding='max_length', truncation=True, max_length=max_length, return_tensors='pt')\n",
    "#         tokenized_data.append((label, tokens['input_ids'].squeeze(0)))\n",
    "#     return tokenized_data\n",
    "\n",
    "# train = tokenize_data(train, tokenizer)\n",
    "# test = tokenize_data(test, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %env CUDA_LAUNCH_BLOCKING=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 50/50 [08:59<00:00, 10.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6281536817550659, 0.3914911448955536, 0.2477555125951767, 0.44476237893104553, 0.5029935836791992, 0.23210160434246063, 0.2422540932893753, 0.10572236031293869, 0.44623589515686035, 0.377618670463562, 0.4931173324584961, 0.2590349614620209, 0.619849681854248, 0.30617913603782654, 0.0948827937245369, 0.13597002625465393, 0.2991216778755188, 0.3802453577518463, 0.33251169323921204, 0.16540858149528503, 0.0673525407910347, 0.025242848321795464, 0.015696093440055847, 0.0271279439330101, 0.04158104211091995, 0.00987352803349495, 0.005758997518569231, 0.009109395556151867, 0.0051191020756959915, 0.6534697413444519, 0.04203469678759575, 0.004979562945663929, 0.006617422681301832, 0.004884060472249985, 0.015144217759370804, 0.0028130970895290375, 0.002308955416083336, 0.021797845140099525, 0.005704130977392197, 0.008588522672653198, 0.0018409698968753219, 0.03512774035334587, 0.0011812576558440924, 0.0023488670121878386, 0.004884304478764534, 0.005404841620475054, 0.0014519558753818274, 0.0015975965652614832, 0.002192690037190914, 0.10134577006101608]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "device='cpu'\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "import importlib\n",
    "import my_transformers\n",
    "importlib.reload(my_transformers)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = my_transformers.ClassificationTransformer(k=4, heads=4, depth=2, seq_length=mx, num_tokens=tokenizer.vocab_size, num_classes=2)\n",
    "model = model.to(device)\n",
    "opt = torch.optim.Adam(lr=0.001, params=model.parameters())\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "model.train()\n",
    "epochs = 50\n",
    "losses = []\n",
    "\n",
    "for _ in tqdm.tqdm(range(epochs)):\n",
    "    for loader in train_loader:\n",
    "        # print(loader['labels'], loader['tokens'])\n",
    "        labels, tokens = loader['labels'].to(device), loader['tokens'].to(device)\n",
    "        # labels -= 1\n",
    "        \n",
    "        opt.zero_grad()\n",
    "        out = model(tokens)\n",
    "        loss = criterion(out, labels)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "    losses.append(loss.item())\n",
    "\n",
    "print(losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6281536817550659, 0.3914911448955536, 0.2477555125951767, 0.44476237893104553, 0.5029935836791992, 0.23210160434246063, 0.2422540932893753, 0.10572236031293869, 0.44623589515686035, 0.377618670463562, 0.4931173324584961, 0.2590349614620209, 0.619849681854248, 0.30617913603782654, 0.0948827937245369, 0.13597002625465393, 0.2991216778755188, 0.3802453577518463, 0.33251169323921204, 0.16540858149528503, 0.0673525407910347, 0.025242848321795464, 0.015696093440055847, 0.0271279439330101, 0.04158104211091995, 0.00987352803349495, 0.005758997518569231, 0.009109395556151867, 0.0051191020756959915, 0.6534697413444519, 0.04203469678759575, 0.004979562945663929, 0.006617422681301832, 0.004884060472249985, 0.015144217759370804, 0.0028130970895290375, 0.002308955416083336, 0.021797845140099525, 0.005704130977392197, 0.008588522672653198, 0.0018409698968753219, 0.03512774035334587, 0.0011812576558440924, 0.0023488670121878386, 0.004884304478764534, 0.005404841620475054, 0.0014519558753818274, 0.0015975965652614832, 0.002192690037190914, 0.10134577006101608]\n"
     ]
    }
   ],
   "source": [
    "print(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.73908\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    total_pred = 0\n",
    "    total_correct = 0\n",
    "\n",
    "    for loader in test_laoder:\n",
    "        labels, tokens = loader['labels'].to(device), loader['tokens'].to(device)\n",
    "        out = model(tokens)\n",
    "        _, pred = torch.max(out, 1)\n",
    "        total_pred += labels.shape[0]\n",
    "\n",
    "        total_correct += (pred == labels).sum().item()\n",
    "\n",
    "        # i = 30\n",
    "        # print(tokenizer.decode(tokens[i]))\n",
    "        # print(labels[i])\n",
    "        # 0 neg\n",
    "\n",
    "print(total_correct/total_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive\n"
     ]
    }
   ],
   "source": [
    "\n",
    "review_text = \"Toy story is a movie, or is it? \"\n",
    "with torch.no_grad():\n",
    "    review_token = tokenizer(review_text, padding='max_length', truncation=True, max_length=100, return_tensors='pt')['input_ids'][0]\n",
    "    review_token = review_token.to(device)\n",
    "    out = model(review_token.view(1, -1))\n",
    "    _, pred = torch.max(out, 1)\n",
    "\n",
    "if pred.item() == 0:\n",
    "    print('negative')\n",
    "else:\n",
    "    print('positive')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
